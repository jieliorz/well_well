

# model
init_train:  True
keep_prob: 0.7
hidden_size: 128
embedding_size: 128
learning_rate: 0.002
batch_size: 32
num_epochs: 10
# Input params
default_batch_size: 1024  # Maximum number of tokens per batch of examples.
max_length: 256  # Maximum number of tokens per example.
# Model params
initializer_gain: 1.0  # Used in trainable variable initialization.
vocab_size: 33708  # Number of tokens defined in the vocabulary file.
hidden_size: 32  # Model dimension in the hidden layers.
num_hidden_layers: 6  # Number of layers in the encoder and decoder stacks.
num_heads: 4  # Number of heads to use in multi-headed attention.
filter_size: 256  # Inner layer dimension in the feedforward network.
# Dropout values (only used when training)
layer_postprocess_dropout: 0.1
attention_dropout: 0.1
relu_dropout: 0.1
# Training params
label_smoothing: 0.1
learning_rate: 2.0
learning_rate_decay_rate: 1.0
learning_rate_warmup_steps: 16000
# Optimizer params
optimizer_adam_beta1: 0.9
optimizer_adam_beta2: 0.997
optimizer_adam_epsilon: 1e-09
# Default prediction params
extra_decode_length: 50
beam_size: 4
alpha: 0.6  # used to calculate length normalization in beam search






